---
title: "Day twelve - 7/25"
output: 
  html_document:
  keep_md: yes
author: "Marcus W. Beck, beck.marcus@epa.gov"
---

### Spatial analysis in R - interpolation and raster extract

```{r setup}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(stringsAsFactors = FALSE)

library(tidyr)
library(rgdal)
library(rgeos)
library(raster)
library(gstat)
library(ncdf4)
library(ggmap)
```

Load some data:
```{r}
# get data from figshare
# download.file('https://figshare.com/articles/OSS_data_-_2017_monday/5136289', destfile = './tuesdata.zip')
unzip('tuesdata.zip', exdir = './GISdata')
```

#### Extract from a raster

Reproject, crop raster by study area:
```{r}
# import data
study_area <- readOGR('GISdata/study-area/study-area.shp')
climate <- raster('GISdata/climate/air.sfc.mon.mean.nc')
crs(study_area)
crs(climate)

# reproject raster to crs of vector
climate_geog <-  projectRaster(climate, crs = crs(study_area))
plot(climate_geog)

# crop raster by vector
climate_geog_cr <- crop(climate_geog, study_area)
plot(climate_geog_cr)
```

Import point data to extract:
```{r}
slv <- readOGR('GISdata/tues-data/sea_level_2000.shp')
crs(slv)
plot(climate_geog_cr)
plot(slv, add = TRUE)
```

Extract climate raster values based on locations in slv
```{r}
# extract using buffer, not just 1:1 location, summarize w/ mean
# export as a spatial points dataframe, sp = TRUE
climate_mean <- raster::extract(climate_geog_cr, 
                                slv, 
                                buffer = 0.5, 
                                fun = mean, 
                                sp = TRUE
                                )
class(climate_mean)

# get data by slot
climate_df <- climate_mean@data

# get data by indexing (even though it's not a data frame)
climate_mean$year
```

Make a plot:
```{r}
plot(climate_geog_cr, main = 'Mean surface temp at study site')
plot(climate_mean, 
     add = TRUE, 
     pch = 16, 
     cex = scales::rescale(climate_mean$Monthly.air.temperature.at.Surface, to = c(1, 3))
     )
```

Write to file:
```{r eval = T}
writeOGR(climate_mean, 
         layer = 'climate_mean', # file name
         dsn = 'GISdata/tues-data', # path
         driver = 'ESRI Shapefile', 
         overwrite_layer = TRUE
         )
```

### Creating raster data and interpolation

[general overview](http://neondataskills.org/spatial-data/spatial-interpolation-basics)

Creating raster - gridding (just putting a grid over points, grid cells w/o a point are NA), interpolation (deterministic or probabilistic), triangular irregular network (TIN)

We are going to make a raster out of this:

```{r}
sea_level_2000 <- read.csv('GISdata/tues-data/sea_lev_2000.csv')
str(sea_level_2000)
sea_level_2000$elev_mm

# interpolation fails with NA values
# remove them
sea_level_2000 <- sea_level_2000 %>% 
  drop_na(elev_mm)
```

Create the spatial object:
```{r}
# create spatial points
sl <- sea_level_2000
coordinates(sl) <- ~ long + lat
plot(sl)
```

Now we create an empty grid to interpolate the spatial object:
```{r}
long <- seq(-99, -80, by = .1)
lat <- seq(24, 32, by = .1)
grd <- crossing(long, lat)

# convert grid to matrix, then spatial pixels data frame
coordinates(grd) <- ~long + lat # spatial points
gridded(grd) <- TRUE # make a raster

# plot
plot(sl)
plot(grd, add = T)
```

IDW interpolation:
```{r}
# two parameters to consider
# radius - maximum distance beyond which not to interpolate beyond each point
# power - how much it weights points by distance, e.g., weighting points higher that are farther away makes a smoother surface (higher power)
idw_pow1 <- idw(formula = elev_mm ~ 1, 
                locations = sl, # data used for interpolation
                newdata = grd, # new data to interpolate
                idp = 1, # power 
                maxdist = 100
                )
plot(idw_pow1)
```

## Reading netcdf and other open data formats - hierarchical

[info from NEON](https://support.hdfgroup.org/HDF5/)

[Bryce's link](http://brycemecum.com/2014/02/18/working-with-netcdf-files-in-r.html)

HDF - hierarchical data format, they store different types of data.  Embedded metadata. 

[HDFview](https://support.hdfgroup.org/HDF5/) - a lightweight Java application for viewing structure of HDF files.

CF conventions - tagging of metadata for processing with other software. 

NetCDF is a wrapper to HDF5. 

Can store images, text, etc. 

Different from a database?  HDF4, NetCDF are similar but archivable.  Text file access/speed work better with databases but it's not a place to archive. 

ncdump - program used by NCEAS for getting metadata from netcdf.  

